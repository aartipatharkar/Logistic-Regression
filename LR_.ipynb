{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1)What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "->\n",
        "Logistic Regression is used for classification problems to predict the probability of a categorical outcome (like \"yes\" or \"no\"), often using a sigmoid function to produce an S-shaped curve. In contrast, Linear Regression is used for regression problems to predict a continuous numerical outcome (like sales or temperature) by modeling a linear relationship between variables. The key difference lies in the type of problem they solve: logistic for classification, linear for regression."
      ],
      "metadata": {
        "id": "vKIYr0tCHQFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Explain the role of the Sigmoid function in Logistic Regression.\n",
        "->\n",
        "\n",
        "In Logistic Regression, the Sigmoid (or logistic) function serves a crucial role by transforming the output of a linear model into a probability between 0 and 1, making it suitable for binary classification problems. Its \"S-shaped\" curve squashes any real-valued input into this specific probability range, enabling the model to predict the likelihood of an event. The sigmoid function also provides a differentiable output, which is essential for the gradient descent algorithm used to train the logistic regression model."
      ],
      "metadata": {
        "id": "-Mvi7hZgHj5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)What is Regularization in Logistic Regression and why is it needed?\n",
        "->\n",
        "Regularization in Logistic Regression is a technique to prevent overfitting by adding a penalty term to the model's loss function, discouraging overly complex models with extreme parameter values. It is needed because without it, logistic regression can drive the loss to zero by fitting the noise in the training data too closely, leading to poor performance on new, unseen data. Regularization helps improve the model's ability to generalize by finding a balance between fitting the training data and maintaining model simplicity."
      ],
      "metadata": {
        "id": "DhWLb5aoIAzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "->\n",
        "Common classification evaluation metrics include Accuracy, Precision, Recall, F1-Score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC). These metrics are important because they provide a quantitative measure of a model's performance by quantifying different aspects of its predictions, such as the ratio of correct to incorrect classifications (Accuracy), the reliability of positive predictions (Precision), and the model's ability to identify all relevant cases (Recall). This allows for informed decisions regarding model selection, hyperparameter tuning, and ensuring the model aligns with specific business or scientific goals.\n",
        "\n",
        "\n",
        "Common classification evaluation metrics include Accuracy, Precision, Recall, F1-Score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC). These metrics are important because they provide a quantitative measure of a model's performance by quantifying different aspects of its predictions, such as the ratio of correct to incorrect classifications (Accuracy), the reliability of positive predictions (Precision), and the model's ability to identify all relevant cases (Recall). This allows for informed decisions regarding model selection, hyperparameter tuning, and ensuring the model aligns with specific business or scientific goals.\n",
        "Common Evaluation Metrics\n",
        "\n",
        "Accuracy\n",
        ": The ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances.\n",
        "Importance: A straightforward measure of overall performance, but can be misleading on imbalanced datasets.\n",
        "\n",
        "Precision\n",
        ": The ratio of true positives (correctly predicted positives) to all instances predicted as positive.\n",
        "Importance: Useful when minimizing false positives is crucial, such as in scenarios where a positive prediction carries a high cost.\n",
        "\n",
        "Recall\n",
        "(or Sensitivity): The ratio of true positives to all actual positive instances in the dataset.\n",
        "Importance: Important when minimizing false negatives is paramount, such as in medical diagnoses where failing to detect a condition can be very costly.\n",
        "\n",
        "F1-Score\n",
        ": A single metric that provides a harmonic mean of precision and recall, balancing the two.\n",
        "Importance: Provides a unified view of a model's performance on imbalanced datasets by considering both precision and recall.\n",
        "\n",
        "AUC-ROC\n",
        "(Area Under the Receiver Operating Characteristic Curve): A single number summarizing the performance of a classification model across all possible thresholds. The ROC curve itself plots the true positive rate (recall) against the false positive rate.\n",
        "Importance: A robust metric, especially for imbalanced datasets, that measures a model's ability to distinguish between classes.\n"
      ],
      "metadata": {
        "id": "6VEiQ7B-Iczc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n"
      ],
      "metadata": {
        "id": "09zB3tiyJnxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset from sklearn)\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df[iris.feature_names]\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M27OqbbnJxkk",
        "outputId": "db4eeee3-5cbd-4942-ee36-fa0adf42bfd2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Logistic Regression model using L2\n",
        "#regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris)\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df[iris.feature_names]\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with L2 regularization (default = 'l2')\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='auto', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Logistic Regression with L2 regularization (Ridge)\")\n",
        "print(\"Coefficients:\\n\", model.coef_)\n",
        "print(\"Intercept:\\n\", model.intercept_)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kEnvwD6KCZt",
        "outputId": "81930fb3-81f9-4f9c-d784-13708b0af944"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 regularization (Ridge)\n",
            "Coefficients:\n",
            " [[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "Intercept:\n",
            " [  9.00884295   1.86902164 -10.87786459]\n",
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset (Iris dataset)\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df[iris.feature_names]\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression with one-vs-rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Logistic Regression with OvR (One-vs-Rest)\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5PJxES2KPK_",
        "outputId": "6ce2fb58-2880-4439-9e26-f498db3bbdfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with OvR (One-vs-Rest)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset (Iris)\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df[iris.feature_names]\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=500, solver='liblinear')\n",
        "# 'liblinear' supports both l1 and l2 penalties\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],       # Regularization strength\n",
        "    'penalty': ['l1', 'l2']             # Penalty types\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Accuracy:\", grid_search.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-qivsYOKZ8l",
        "outputId": "ef4278c8-f13c-4dda-e7c4-a3f170a3d44a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9583333333333334\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris)\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df[iris.feature_names]\n",
        "y = df['target']\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=500)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=500)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Results\n",
        "print(\"Logistic Regression Accuracy (without scaling):\", accuracy_no_scaling)\n",
        "print(\"Logistic Regression Accuracy (with scaling):   \", accuracy_with_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEFkymSyKkWW",
        "outputId": "75ce244c-ae33-4338-9075-7c3637ffe487"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy (without scaling): 1.0\n",
            "Logistic Regression Accuracy (with scaling):    1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load your dataset\n",
        "# (Here we just simulate with sklearn dataset for demo purposes)\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=5000, n_features=20,\n",
        "                           n_classes=2, weights=[0.95, 0.05],\n",
        "                           random_state=42)\n",
        "\n",
        "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
        "df['target'] = y\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Train/Test Split (stratified to preserve imbalance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(\"target\", axis=1),\n",
        "    df[\"target\"],\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"target\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Build Pipeline: Scaling + SMOTE + Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"smote\", SMOTE(random_state=42)),\n",
        "    (\"logreg\", LogisticRegression(solver=\"saga\", max_iter=5000))\n",
        "])\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Define Hyperparameter Grid\n",
        "param_grid = {\n",
        "    \"logreg__C\": [0.01, 0.1, 1, 10],\n",
        "    \"logreg__penalty\": [\"l1\", \"l2\"]\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Grid Search with Stratified CV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"roc_auc\",   # focus on ranking positives higher\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Evaluate on Test Set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"Test PR-AUC:\", average_precision_score(y_test, y_prob))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fARB7IXKwyV",
        "outputId": "81df37db-7346-4ec8-e5d8-18402725cf1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'logreg__C': 0.01, 'logreg__penalty': 'l1'}\n",
            "Test ROC-AUC: 0.8575853775853776\n",
            "Test PR-AUC: 0.48348181189907835\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92       945\n",
            "           1       0.25      0.76      0.38        55\n",
            "\n",
            "    accuracy                           0.86      1000\n",
            "   macro avg       0.62      0.81      0.65      1000\n",
            "weighted avg       0.94      0.86      0.89      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_n_00rUWLCDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}